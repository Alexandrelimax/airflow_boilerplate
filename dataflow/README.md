# ğŸš€ Dataflow Boilerplate (Apache Airflow + Apache Beam com Flex Template + BigQuery + Terraform)

Este exemplo mostra como criar um pipeline com Apache Beam, empacotar como Docker, gerar um Flex Template e executÃ¡-lo no Dataflow.  

O pipeline:
- LÃª um arquivo CSV do Cloud Storage (GCS)
- Converte os nomes para letras maiÃºsculas
- Salva os dados em uma tabela no BigQuery

Inclui:

- Pipeline Apache Beam (`pipeline.py`)
- `Dockerfile` com base no SDK oficial do Beam
- `metadata.json` com parÃ¢metros do template
- Script `build_dataflow.sh` para build e upload do template
- Infraestrutura com Terraform para criar dataset, tabela e executar o job
- Duas DAGs Airflow de exemplo:
  - Usando operador clÃ¡ssico
  - Usando sintaxe moderna com `@task` e `DataflowHook`

---

## ğŸ“ Estrutura

```bash
dataflow/
â”œâ”€â”€ dag/
â”‚   â”œâ”€â”€ classic_operator_dag.py     # DAG com operador clÃ¡ssico
â”‚   â””â”€â”€ modern_taskflow_dag.py      # DAG com @task e Hook
â”œâ”€â”€ terraform/                      # Infraestrutura com Terraform
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ terraform.tfvars
â”‚   â””â”€â”€ outputs.tf
â”œâ”€â”€ build.sh                        # Script que builda e envia imagem/template
â”œâ”€â”€ Dockerfile                      # Imagem base Beam SDK
â”œâ”€â”€ metadata.json                   # ParÃ¢metros do Flex Template
â”œâ”€â”€ pipeline.py                     # CÃ³digo do pipeline Apache Beam
â”œâ”€â”€ requirements.txt                # DependÃªncias do pipeline
â””â”€â”€ README.md
```

---

## ğŸ“¦ PrÃ©-requisitos

- Projeto GCP com as seguintes APIs ativadas:
  - Cloud Storage
  - BigQuery
  - Dataflow
  - Artifact Registry

- Buckets criados manualmente:
  - Um bucket para armazenar o CSV  
    Ex: `gs://bucket/csv/data.csv`
  - Um bucket para armazenar o Flex Template  
    Ex: `gs://<project-id>-dataflow-templates`

---

## ğŸ› ï¸ Build e Deploy do Template

1. Edite o script `build_dataflow.sh` com:
   - `PROJECT_ID`
   - `REGION`
   - `ARTIFACT_ID`
   - `IMAGE_NAME`

2. Execute:

```bash
chmod +x build.sh
./build.sh

```
O script irÃ¡:

 - Construir a imagem Docker do pipeline

 - Fazer push para o Artifact Registry

 - Gerar o Flex Template no GCS com base no metadata.json

## â˜ï¸ Executar via Terraform

1. Configure o arquivo `terraform.tfvars` com seus valores:

```hcl
project_id         = "meu-projeto-id"
region             = "us-central1"
bq_dataset_id      = "dataset_teste"
bq_table_id        = "pessoas"
bq_schema_file     = "./schemas/pessoas_schema.json"
template_bucket    = "meu-bucket-templates"
template_path      = "templates/uppercase-dataflow-template.json"
gcs_bucket_name    = "bucket-teste-saida"
gcs_input_path     = "csv/data.csv"
dataflow_job_name  = "job-pessoas-uppercase"
```
Execute:

```bash
cd terraform
terraform init
terraform apply -var-file="terraform.tfvars"
```

## â˜ï¸ Executar via Console (sem Terraform)

VocÃª tambÃ©m pode executar o Flex Template manualmente atravÃ©s do Console do GCP:

1. Acesse **Dataflow > Criar job**
2. Selecione **Modelo personalizado**
3. Informe o caminho do template:  
   `gs://meu-bucket-templates/templates/uppercase-dataflow-template.json`
4. Preencha os parÃ¢metros:

   ```text
   input: gs://bucket-teste-saida/csv/data.csv
   output_table: dataset_teste.pessoas
   ```

## â˜ï¸ Executar via Airflow

### âœ… ClÃ¡ssico

**Arquivo:** `dag/classic_operator_dag.py`

- Utiliza o operador `DataflowFlexTemplateOperator`
- Boa opÃ§Ã£o para quem prefere a abordagem declarativa tradicional

---

### âœ… Moderno

**Arquivo:** `dag/modern_taskflow_dag.py`

- Utiliza `@task` com `DataflowHook` e `start_flex_template`
- Recomendado para quem busca mais flexibilidade, reutilizaÃ§Ã£o de cÃ³digo e controle de fluxo dinÃ¢mico

---

## âœ¨ PersonalizaÃ§Ãµes

- Edite o arquivo `pipeline.py` para modificar a transformaÃ§Ã£o de dados
- Altere o `pessoas_schema.json` para incluir ou remover colunas
- Atualize o `metadata.json` para adicionar novos parÃ¢metros ao Flex Template
- Ajuste a forma de escrita no BigQuery:

  ```python
  write_disposition = beam.io.BigQueryDisposition.WRITE_APPEND
  ```
