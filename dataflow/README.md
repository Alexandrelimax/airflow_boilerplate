# ğŸš€ Dataflow Boilerplate (Apache Beam + BigQuery)

Este exemplo mostra como criar um pipeline com Apache Beam, rodando no Dataflow, que:
- LÃª um arquivo CSV do Cloud Storage (GCS)
- Realiza uma transformaÃ§Ã£o simples (ex: deixa o nome em maiÃºsculas)
- Salva os dados no BigQuery

Inclui:
- Pipeline Apache Beam (`pipeline/main.py`)
- Empacotamento com `setup.py`
- Script de execuÃ§Ã£o (`run.sh`)
- Duas versÃµes de DAG do Airflow:
  - Usando operador clÃ¡ssico (`classic_operator_dag.py`)
  - Usando sintaxe moderna com `@task` e `DataflowHook` (`modern_taskflow_dag.py`)

---

## ğŸ“ Estrutura

```bash
dataflow/
 â”œâ”€â”€ dags/ â”‚ 
 â”œâ”€â”€ classic_operator_dag.py # DAG usando operador Dataflow clÃ¡ssico 
 â”‚ â””â”€â”€ modern_taskflow_dag.py # DAG usando @task e DataflowHook 
 â”œâ”€â”€ pipeline/ 
 â”‚ â””â”€â”€ main.py # CÃ³digo do pipeline Beam 
 â”œâ”€â”€ requirements.txt 
 â”œâ”€â”€ setup.py # Empacotamento pro Dataflow 
 â”œâ”€â”€ run.sh # Executa local ou envia pro Dataflow 
 â””â”€â”€ README.md
```


---

## ğŸ“¦ PrÃ©-requisitos

- Projeto no GCP com APIs habilitadas:
  - Cloud Storage
  - BigQuery
  - Dataflow
- Bucket no GCS criado com:
  - `input/data.csv` (com cabeÃ§alho: `id,name,age`)
  - Pasta `temp/` para staging
  - Pasta `dataflow/` para salvar o `main.py` se for usar no Airflow
- Dataset e tabela no BigQuery com schema:
  - `id:INTEGER`, `name:STRING`, `age:INTEGER`

---

## â–¶ï¸ Executar via terminal (sem Airflow)

1. Ajuste o `run.sh` com seu:
   - `PROJECT_ID`
   - `REGION`
   - `BUCKET_NAME`
   - `DATASET` e `TABLE`

2. Rode o script:
```bash
bash run.sh

```

â˜ï¸ **Executar via Airflow**

âœ… **OpÃ§Ã£o 1: Usando operador clÃ¡ssico**  
`Arquivo: dags/classic_operator_dag.py`  

- Usa `DataflowCreatePythonJobOperator`  
- Requer que o `main.py` esteja no GCS: `gs://<bucket>/dataflow/main.py`  

âœ… **OpÃ§Ã£o 2: Usando sintaxe moderna (@task + hook)**  
`Arquivo: dags/modern_taskflow_dag.py`  

- Usa `@task` com `DataflowHook`  
- Mais flexÃ­vel e reutilizÃ¡vel  

âœ¨ **PersonalizaÃ§Ãµes**  
- Mude o nome das colunas e lÃ³gica de transformaÃ§Ã£o no `pipeline/main.py`  
- Alterne entre `WriteToBigQueryDisposition.WRITE_APPEND` ou `WRITE_TRUNCATE`  
- Adicione validaÃ§Ãµes, filtros, joins, etc.  

âœ… **Dica final**  
Este boilerplate Ã© a base. Copie e cole como template para novos pipelines!